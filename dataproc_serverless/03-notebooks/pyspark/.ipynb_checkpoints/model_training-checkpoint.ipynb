{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING\n",
    "This script does training with Spark MLLib of a Random Forest Classification model for the customer churn prediction experiment-</br>\n",
    "Uses BigQuery as a source, and writes test results, model metrics and \n",
    "feature importance scores to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import pandas as pd\n",
    "import sys, logging, argparse, random, tempfile, json\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "from pathlib import Path as path\n",
    "from google.cloud import storage\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gdpic-srvls-session-1d836424-49f7-43ae-bd98-9bbd5751e514-m.us-central1-a.c.vertex-ai-382806.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://gdpic-srvls-session-1d836424-49f7-43ae-bd98-9bbd5751e514-m:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fee36fb50a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1a. Arguments\n",
    "pipelineID = random.randint(1, 10000)\n",
    "projectNbr = \"433578906282\"\n",
    "projectID = \"vertex-ai-382806\"\n",
    "displayPrintStatements = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1b. Variables \n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"training\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "modelBaseNm = appBaseName\n",
    "modelVersion = pipelineID\n",
    "bqDatasetNm = f\"{projectID}.customer_churn_ds\"\n",
    "operation = appNameSuffix\n",
    "bigQuerySourceTableFQN = f\"{bqDatasetNm}.training_data\"\n",
    "bigQueryModelTestResultsTableFQN = f\"{bqDatasetNm}.test_predictions\"\n",
    "bigQueryModelMetricsTableFQN = f\"{bqDatasetNm}.model_metrics\"\n",
    "bigQueryFeatureImportanceTableFQN = f\"{bqDatasetNm}.model_feature_importance_scores\"\n",
    "modelBucketUri = f\"gs://s8s_model_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "metricsBucketUri = f\"gs://s8s_metrics_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "scratchBucketUri = f\"s8s-spark-bucket-{projectNbr}/{appBaseName}/pipelineId-{pipelineID}/{appNameSuffix}/\"\n",
    "pipelineExecutionDt = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other variables, constants\n",
    "SPLIT_SEED = 6\n",
    "SPLIT_SPECS = [0.8, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training for *Customer Churn* experiment\n",
      ".....................................................\n",
      "The datetime now is - 20240603065934\n",
      " \n",
      "INPUT PARAMETERS\n",
      "....pipelineID=4244\n",
      "....projectID=vertex-ai-382806\n",
      "....projectNbr=433578906282\n",
      "....displayPrintStatements=True\n",
      " \n",
      "EXPECTED SETUP\n",
      "....BQ Dataset=vertex-ai-382806.customer_churn_ds\n",
      "....Model Training Source Data in BigQuery=vertex-ai-382806.customer_churn_ds.training_data\n",
      "....Scratch Bucket for BQ connector=gs://s8s-spark-bucket-433578906282\n",
      "....Model Bucket=gs://s8s-model-bucket-433578906282\n",
      "....Metrics Bucket=gs://s8s-metrics-bucket-433578906282\n",
      " \n",
      "OUTPUT\n",
      "....Model in GCS=gs://s8s_model_bucket-433578906282/customer-churn-model/training/4244\n",
      "....Model metrics in GCS=gs://s8s_metrics_bucket-433578906282/customer-churn-model/training/4244\n",
      "....Model metrics in BigQuery=vertex-ai-382806.customer_churn_ds.model_metrics\n",
      "....Model feature importance scores in BigQuery=vertex-ai-382806.customer_churn_ds.model_feature_importance_scores\n",
      "....Model test results in BigQuery=vertex-ai-382806.customer_churn_ds.test_predictions\n"
     ]
    }
   ],
   "source": [
    "# 1c. Display input and output\n",
    "if displayPrintStatements:\n",
    "    print(\"Starting model training for *Customer Churn* experiment\")\n",
    "    print(\".....................................................\")\n",
    "    print(f\"The datetime now is - {pipelineExecutionDt}\")\n",
    "    print(\" \")\n",
    "    print(\"INPUT PARAMETERS\")\n",
    "    print(f\"....pipelineID={pipelineID}\")\n",
    "    print(f\"....projectID={projectID}\")\n",
    "    print(f\"....projectNbr={projectNbr}\")\n",
    "    print(f\"....displayPrintStatements={displayPrintStatements}\")\n",
    "    print(\" \")\n",
    "    print(\"EXPECTED SETUP\")  \n",
    "    print(f\"....BQ Dataset={bqDatasetNm}\")\n",
    "    print(f\"....Model Training Source Data in BigQuery={bigQuerySourceTableFQN}\")\n",
    "    print(f\"....Scratch Bucket for BQ connector=gs://s8s-spark-bucket-{projectNbr}\") \n",
    "    print(f\"....Model Bucket=gs://s8s-model-bucket-{projectNbr}\")  \n",
    "    print(f\"....Metrics Bucket=gs://s8s-metrics-bucket-{projectNbr}\") \n",
    "    print(\" \")\n",
    "    print(\"OUTPUT\")\n",
    "    print(f\"....Model in GCS={modelBucketUri}\")\n",
    "    print(f\"....Model metrics in GCS={metricsBucketUri}\")  \n",
    "    print(f\"....Model metrics in BigQuery={bigQueryModelMetricsTableFQN}\")      \n",
    "    print(f\"....Model feature importance scores in BigQuery={bigQueryFeatureImportanceTableFQN}\") \n",
    "    print(f\"....Model test results in BigQuery={bigQueryModelTestResultsTableFQN}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Initializing spark & spark configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 06:59:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# 2. Spark Session creation\n",
    "print('....Initializing spark & spark configs')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "\n",
    "# Spark configuration setting for writes to BigQuery\n",
    "spark.conf.set(\"parentProject\", projectID)\n",
    "spark.conf.set(\"temporaryGcsBucket\", scratchBucketUri)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Add Python modules\n",
    "sc.addPyFile(f\"gs://s8s_code_bucket-{projectNbr}/pyspark/common_utils.py\")\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING DATA - READ, SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Read the training dataset into a dataframe\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- senior_citizen: long (nullable = true)\n",
      " |-- partner: string (nullable = true)\n",
      " |-- dependents: string (nullable = true)\n",
      " |-- tenure: long (nullable = true)\n",
      " |-- tenure_group: string (nullable = true)\n",
      " |-- phone_service: string (nullable = true)\n",
      " |-- multiple_lines: string (nullable = true)\n",
      " |-- internet_service: string (nullable = true)\n",
      " |-- online_security: string (nullable = true)\n",
      " |-- online_backup: string (nullable = true)\n",
      " |-- device_protection: string (nullable = true)\n",
      " |-- tech_support: string (nullable = true)\n",
      " |-- streaming_tv: string (nullable = true)\n",
      " |-- streaming_movies: string (nullable = true)\n",
      " |-- contract: string (nullable = true)\n",
      " |-- paperless_billing: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- monthly_charges: double (nullable = true)\n",
      " |-- total_charges: double (nullable = true)\n",
      " |-- churn: string (nullable = true)\n",
      " |-- pipeline_id: string (nullable = false)\n",
      " |-- pipeline_execution_dt: string (nullable = false)\n",
      "\n",
      "inputDF count=7032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. Read training data\n",
    "print('....Read the training dataset into a dataframe')\n",
    "inputDF = spark.read \\\n",
    "    .format('bigquery') \\\n",
    "    .load(bigQuerySourceTableFQN)\n",
    "\n",
    "inputDF.printSchema()\n",
    "\n",
    "if displayPrintStatements:\n",
    "    print(f\"inputDF count={inputDF.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Typecast some columns to the right datatype\n",
    "inputDF = inputDF.withColumn(\"partner\", inputDF.partner.cast('string')) \\\n",
    "    .withColumn(\"dependents\", inputDF.dependents.cast('string')) \\\n",
    "    .withColumn(\"phone_service\", inputDF.phone_service.cast('string')) \\\n",
    "    .withColumn(\"paperless_billing\", inputDF.paperless_billing.cast('string')) \\\n",
    "    .withColumn(\"churn\", inputDF.churn.cast('string')) \\\n",
    "    .withColumn(\"monthly_charges\", inputDF.monthly_charges.cast('float')) \\\n",
    "    .withColumn(\"total_charges\", inputDF.total_charges.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Split the dataset\n"
     ]
    }
   ],
   "source": [
    "# 4. Split to training and test datasets\n",
    "print('....Split the dataset')\n",
    "trainDF, testDF = inputDF.randomSplit(SPLIT_SPECS, seed=SPLIT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING & FEATURE ENGINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Data pre-procesing\n"
     ]
    }
   ],
   "source": [
    "# 5. Pre-process training data\n",
    "print('....Data pre-procesing')\n",
    "dataPreprocessingStagesList = []\n",
    "# 5a. Create and append to pipeline stages - string indexing and one hot encoding\n",
    "for eachCategoricalColumn in common_utils.CATEGORICAL_COLUMN_LIST:\n",
    "    # Category indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=eachCategoricalColumn, outputCol=eachCategoricalColumn + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[eachCategoricalColumn + \"classVec\"])\n",
    "    # Add stages.  This is a lazy operation\n",
    "    dataPreprocessingStagesList += [stringIndexer, encoder]\n",
    "\n",
    "# 5b. Convert label into label indices using the StringIndexer and append to pipeline stages\n",
    "labelStringIndexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "dataPreprocessingStagesList += [labelStringIndexer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Feature engineering\n"
     ]
    }
   ],
   "source": [
    "# 6. Feature engineering\n",
    "print('....Feature engineering')\n",
    "featureEngineeringStageList = []\n",
    "assemblerInputs = common_utils.NUMERIC_COLUMN_LIST + [c + \"classVec\" for c in common_utils.CATEGORICAL_COLUMN_LIST]\n",
    "featuresVectorAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "featureEngineeringStageList += [featuresVectorAssembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Model training\n"
     ]
    }
   ],
   "source": [
    "# 5. Model training\n",
    "print('....Model training')\n",
    "modelTrainingStageList = []\n",
    "rfClassifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "modelTrainingStageList += [rfClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Instantiating pipeline model\n"
     ]
    }
   ],
   "source": [
    "# 6. Create a model training pipeline for stages defined\n",
    "print('....Instantiating pipeline model')\n",
    "pipeline = Pipeline(stages=dataPreprocessingStagesList + featureEngineeringStageList + modelTrainingStageList) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Fit the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 9. Fit the model\n",
    "print('....Fit the model')\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mleap.pyspark\n",
    "# from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "# pipelineModel.serializeToBundle(\"jar:file:/tmp/mleap_python_model_export/churn-pipeline-json.zip\", pipelineModel.transform(trainDF))\n",
    "# upload_model_to_gcs('/tmp/mleap_python_model_export/churn-pipeline-json.zip', 'gs://s8s_model_bucket-433578906282/customer-churn-model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def upload_model_to_gcs(local_model_path, gcs_destination):\n",
    "#     \"\"\"\n",
    "#     Uploads a model file from the local filesystem to Google Cloud Storage.\n",
    "\n",
    "#     Args:\n",
    "#         local_model_path (str): The path to the model file on your local machine.\n",
    "#         gcs_destination (str): The GCS URI where you want to store the model.\n",
    "#     \"\"\"\n",
    "#     import subprocess\n",
    "\n",
    "#     command = [\n",
    "#         \"gsutil\",\n",
    "#         \"cp\",\n",
    "#         local_model_path,\n",
    "#         gcs_destination\n",
    "#     ]\n",
    "\n",
    "#     try:\n",
    "#         result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "#         print(f\"Upload successful:\\n{result.stdout}\")\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Upload failed with error:\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Test the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 07:01:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+-------+----------+------+------------+-------------+--------------+----------------+---------------+-------------+-----------------+------------+------------+----------------+--------------+-----------------+----------------+---------------+-------------+-----+-----------+---------------------+-----------+--------------+-------------------+----------------------+------------+---------------+---------------+------------------+------------------+---------------------+-------------------+----------------------+---------------------+------------------------+--------------------+-----------------------+------------------+---------------------+----------------------+-------------------------+-----------------+--------------------+-----------------+--------------------+---------------------+------------------------+-------------+----------------+----------------------+-------------------------+-------------------+----------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|customer_id|gender|senior_citizen|partner|dependents|tenure|tenure_group|phone_service|multiple_lines|internet_service|online_security|online_backup|device_protection|tech_support|streaming_tv|streaming_movies|      contract|paperless_billing|  payment_method|monthly_charges|total_charges|churn|pipeline_id|pipeline_execution_dt|genderIndex|genderclassVec|senior_citizenIndex|senior_citizenclassVec|partnerIndex|partnerclassVec|dependentsIndex|dependentsclassVec|phone_serviceIndex|phone_serviceclassVec|multiple_linesIndex|multiple_linesclassVec|internet_serviceIndex|internet_serviceclassVec|online_securityIndex|online_securityclassVec|online_backupIndex|online_backupclassVec|device_protectionIndex|device_protectionclassVec|tech_supportIndex|tech_supportclassVec|streaming_tvIndex|streaming_tvclassVec|streaming_moviesIndex|streaming_moviesclassVec|contractIndex|contractclassVec|paperless_billingIndex|paperless_billingclassVec|payment_methodIndex|payment_methodclassVec|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----------+------+--------------+-------+----------+------+------------+-------------+--------------+----------------+---------------+-------------+-----------------+------------+------------+----------------+--------------+-----------------+----------------+---------------+-------------+-----+-----------+---------------------+-----------+--------------+-------------------+----------------------+------------+---------------+---------------+------------------+------------------+---------------------+-------------------+----------------------+---------------------+------------------------+--------------------+-----------------------+------------------+---------------------+----------------------+-------------------------+-----------------+--------------------+-----------------+--------------------+---------------------+------------------------+-------------+----------------+----------------------+-------------------------+-------------------+----------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "| 0011-IGKFF|  Male|             1|    Yes|        No|    13|Tenure_12-24|          Yes|            No|     Fiber optic|             No|          Yes|              Yes|          No|         Yes|             Yes|Month-to-month|              Yes|Electronic check|           98.0|      1237.85|  yes|       8452|       20240603063055|        0.0| (1,[0],[1.0])|                1.0|             (1,[],[])|         1.0|      (1,[],[])|            0.0|     (1,[0],[1.0])|               0.0|        (1,[0],[1.0])|                0.0|         (2,[0],[1.0])|                  0.0|           (2,[0],[1.0])|                 0.0|          (1,[0],[1.0])|               1.0|            (1,[],[])|                   1.0|                (1,[],[])|              0.0|       (1,[0],[1.0])|              1.0|           (1,[],[])|                  1.0|               (1,[],[])|          0.0|   (2,[0],[1.0])|                   0.0|            (1,[0],[1.0])|                0.0|         (3,[0],[1.0])|  1.0|(23,[0,1,2,5,6,7,...|[9.23108013239536...|[0.46155400661976...|       1.0|\n",
      "| 0013-EXCHZ|Female|             1|    Yes|        No|     3| Tenure_0-12|          Yes|            No|     Fiber optic|             No|           No|               No|         Yes|         Yes|              No|Month-to-month|              Yes|    Mailed check|           83.9|        267.4|  yes|       8452|       20240603063055|        1.0|     (1,[],[])|                1.0|             (1,[],[])|         1.0|      (1,[],[])|            0.0|     (1,[0],[1.0])|               0.0|        (1,[0],[1.0])|                0.0|         (2,[0],[1.0])|                  0.0|           (2,[0],[1.0])|                 0.0|          (1,[0],[1.0])|               0.0|        (1,[0],[1.0])|                   0.0|            (1,[0],[1.0])|              1.0|           (1,[],[])|              1.0|           (1,[],[])|                  0.0|           (1,[0],[1.0])|          0.0|   (2,[0],[1.0])|                   0.0|            (1,[0],[1.0])|                1.0|         (3,[1],[1.0])|  1.0|(23,[0,1,5,6,7,9,...|[8.54325524837585...|[0.42716276241879...|       1.0|\n",
      "+-----------+------+--------------+-------+----------+------+------------+-------------+--------------+----------------+---------------+-------------+-----------------+------------+------------+----------------+--------------+-----------------+----------------+---------------+-------------+-----+-----------+---------------------+-----------+--------------+-------------------+----------------------+------------+---------------+---------------+------------------+------------------+---------------------+-------------------+----------------------+---------------------+------------------------+--------------------+-----------------------+------------------+---------------------+----------------------+-------------------------+-----------------+--------------------+-----------------+--------------------+---------------------+------------------------+-------------+----------------+----------------------+-------------------------+-------------------+----------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 10. Test the model with the test dataset\n",
    "print('....Test the model')\n",
    "predictionsDF = pipelineModel.transform(testDF)\n",
    "predictionsDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Persist the model to GCS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 11. Persist model to GCS\n",
    "print('....Persist the model to GCS')\n",
    "pipelineModel.write().overwrite().save(modelBucketUri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 12. Persist model testing results to BigQuery\n",
    "persistPredictionsDF = predictionsDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(appNameSuffix)) \n",
    "\n",
    "persistPredictionsDF.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', bigQueryModelTestResultsTableFQN) \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EXPLAINABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(23, {0: 0.1292, 1: 0.2336, 3: 0.0132, 4: 0.0121, 5: 0.0028, 6: 0.0115, 7: 0.0039, 8: 0.0006, 9: 0.1446, 10: 0.0164, 11: 0.0339, 12: 0.0142, 13: 0.0011, 14: 0.0087, 15: 0.0001, 16: 0.0045, 17: 0.1838, 18: 0.0633, 19: 0.0153, 20: 0.0953, 21: 0.0057, 22: 0.0062})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13a. Model explainability - feature importance\n",
    "pipelineModel.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 13b. Function to parse feature importance\n",
    "def fnExtractFeatureImportance(featureImportanceSparseVector, predictionsDataframe, featureColumnListing):\n",
    "    featureColumnMetadataList = []\n",
    "    for i in predictionsDataframe.schema[featureColumnListing].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        featureColumnMetadataList = featureColumnMetadataList + predictionsDataframe.schema[featureColumnListing].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        \n",
    "    featureColumnMetadataPDF = pd.DataFrame(featureColumnMetadataList)\n",
    "    featureColumnMetadataPDF['importance_score'] = featureColumnMetadataPDF['idx'].apply(lambda x: featureImportanceSparseVector[x])\n",
    "    return(featureColumnMetadataPDF.sort_values('importance_score', ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>idx</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>total_charges</td>\n",
       "      <td>1</td>\n",
       "      <td>0.233650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>contractclassVec_Month-to-month</td>\n",
       "      <td>17</td>\n",
       "      <td>0.183778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>internet_serviceclassVec_Fiber optic</td>\n",
       "      <td>9</td>\n",
       "      <td>0.144560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>monthly_charges</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>payment_methodclassVec_Electronic check</td>\n",
       "      <td>20</td>\n",
       "      <td>0.095273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>contractclassVec_Two year</td>\n",
       "      <td>18</td>\n",
       "      <td>0.063349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>online_securityclassVec_No</td>\n",
       "      <td>11</td>\n",
       "      <td>0.033914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>internet_serviceclassVec_DSL</td>\n",
       "      <td>10</td>\n",
       "      <td>0.016387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>paperless_billingclassVec_Yes</td>\n",
       "      <td>19</td>\n",
       "      <td>0.015341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>online_backupclassVec_No</td>\n",
       "      <td>12</td>\n",
       "      <td>0.014167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>senior_citizenclassVec_0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>partnerclassVec_No</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phone_serviceclassVec_Yes</td>\n",
       "      <td>6</td>\n",
       "      <td>0.011527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tech_supportclassVec_No</td>\n",
       "      <td>14</td>\n",
       "      <td>0.008682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>payment_methodclassVec_Credit card (automatic)</td>\n",
       "      <td>22</td>\n",
       "      <td>0.006165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>payment_methodclassVec_Mailed check</td>\n",
       "      <td>21</td>\n",
       "      <td>0.005686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>streaming_moviesclassVec_No</td>\n",
       "      <td>16</td>\n",
       "      <td>0.004544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>multiple_linesclassVec_No</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dependentsclassVec_No</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>device_protectionclassVec_No</td>\n",
       "      <td>13</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>multiple_linesclassVec_Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>streaming_tvclassVec_No</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>genderclassVec_Male</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  idx  importance_score\n",
       "22                                   total_charges    1          0.233650\n",
       "15                 contractclassVec_Month-to-month   17          0.183778\n",
       "7             internet_serviceclassVec_Fiber optic    9          0.144560\n",
       "21                                 monthly_charges    0          0.129179\n",
       "18         payment_methodclassVec_Electronic check   20          0.095273\n",
       "16                       contractclassVec_Two year   18          0.063349\n",
       "9                       online_securityclassVec_No   11          0.033914\n",
       "8                     internet_serviceclassVec_DSL   10          0.016387\n",
       "17                   paperless_billingclassVec_Yes   19          0.015341\n",
       "10                        online_backupclassVec_No   12          0.014167\n",
       "1                         senior_citizenclassVec_0    3          0.013209\n",
       "2                               partnerclassVec_No    4          0.012052\n",
       "4                        phone_serviceclassVec_Yes    6          0.011527\n",
       "12                         tech_supportclassVec_No   14          0.008682\n",
       "20  payment_methodclassVec_Credit card (automatic)   22          0.006165\n",
       "19             payment_methodclassVec_Mailed check   21          0.005686\n",
       "14                     streaming_moviesclassVec_No   16          0.004544\n",
       "5                        multiple_linesclassVec_No    7          0.003940\n",
       "3                            dependentsclassVec_No    5          0.002845\n",
       "11                    device_protectionclassVec_No   13          0.001073\n",
       "6                       multiple_linesclassVec_Yes    8          0.000603\n",
       "13                         streaming_tvclassVec_No   15          0.000078\n",
       "0                              genderclassVec_Male    2          0.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13c. Print feature importance\n",
    "fnExtractFeatureImportance(pipelineModel.stages[-1].featureImportances, predictionsDF, \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 13d. Capture into a Pandas DF\n",
    "featureImportantcePDF = fnExtractFeatureImportance(pipelineModel.stages[-1].featureImportances, predictionsDF, \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------------+-----------+-------------+---------------------+---------+\n",
      "|       feature_index|feature_nm|   importance_score|pipeline_id|model_version|pipeline_execution_dt|operation|\n",
      "+--------------------+----------+-------------------+-----------+-------------+---------------------+---------+\n",
      "|       total_charges|         1|0.23364955448527888|       4244|         4244|       20240603065934| training|\n",
      "|contractclassVec_...|        17|0.18377782190175462|       4244|         4244|       20240603065934| training|\n",
      "+--------------------+----------+-------------------+-----------+-------------+---------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 13e. Persist feature importance scores to BigQuery\n",
    "# Convert Pandas to Spark DF & use Spark to persist\n",
    "featureImportantceDF = spark.createDataFrame(featureImportantcePDF).toDF(\"feature_index\",\"feature_nm\",\"importance_score\")\n",
    "\n",
    "persistFeatureImportanceDF = featureImportantceDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(operation)) \n",
    "\n",
    "persistFeatureImportanceDF.show(2)\n",
    "\n",
    "persistFeatureImportanceDF.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', bigQueryFeatureImportanceTableFQN) \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 14a. Metrics parsing function\n",
    "def fnParseModelMetrics(predictionsDF, labelColumn, operation, boolSubsetOnly):\n",
    "    \"\"\"\n",
    "    Get model metrics\n",
    "    Args:\n",
    "        predictions: predictions\n",
    "        labelColumn: target column\n",
    "        operation: train or test\n",
    "        boolSubsetOnly: boolean for partial(without true, score, prediction) or full metrics \n",
    "    Returns:\n",
    "        metrics: metrics\n",
    "        \n",
    "    Anagha TODO: This function if called from common_utils fails; Need to researchy why\n",
    "    \"\"\"\n",
    "    \n",
    "    metricLabels = ['area_roc', 'area_prc', 'accuracy', 'f1', 'precision', 'recall']\n",
    "    metricColumns = ['true', 'score', 'prediction']\n",
    "    metricKeys = [f'{operation}_{ml}' for ml in metricLabels] + metricColumns\n",
    "\n",
    "    # Instantiate evaluators\n",
    "    bcEvaluator = BinaryClassificationEvaluator(labelCol=labelColumn)\n",
    "    mcEvaluator = MulticlassClassificationEvaluator(labelCol=labelColumn)\n",
    "\n",
    "    # Capture metrics -> areas, acc, f1, prec, rec\n",
    "    area_roc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderROC'}), 5)\n",
    "    area_prc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderPR'}), 5)\n",
    "    acc = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"accuracy\"}), 5)\n",
    "    f1 = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"f1\"}), 5)\n",
    "    prec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedPrecision\"}), 5)\n",
    "    rec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedRecall\"}), 5)\n",
    "\n",
    "    # Get the true, score, prediction off of the test results dataframe\n",
    "    rocDictionary = common_utils.fnGetTrueScoreAndPrediction(predictionsDF, labelColumn)\n",
    "    true = rocDictionary['true']\n",
    "    score = rocDictionary['score']\n",
    "    prediction = rocDictionary['prediction']\n",
    "\n",
    "    # Create a metric values array\n",
    "    metricValuesArray = []\n",
    "    if boolSubsetOnly:\n",
    "        metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec))\n",
    "    else:\n",
    "        metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec, true, score, prediction))\n",
    "    \n",
    "    # Zip the keys and values into a dictionary  \n",
    "    metricsDictionary = dict(zip(metricKeys, metricValuesArray))\n",
    "\n",
    "    return metricsDictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 14b. Capture & display metrics\n",
    "modelMetrics = fnParseModelMetrics(predictionsDF, \"label\", \"test\", True)\n",
    "for m, v in modelMetrics.items():\n",
    "    print(f'{m}: {v}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 14c. Persist metrics subset to GCS\n",
    "blobName = f\"{modelBaseNm}/{operation}/{modelVersion}/subset/metrics.json\"\n",
    "common_utils.fnPersistMetrics(urlparse(metricsBucketUri).netloc, modelMetrics, blobName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 14d. Persist metrics in full to GCS\n",
    "# (The version persisted to BQ does not have True, Score and Prediction needed for Confusion Matrix\n",
    "# This version below has the True, Score and Prediction additionally) \n",
    "\n",
    "# 14d.1. Capture\n",
    "modelMetricsWithTSP = fnParseModelMetrics(predictionsDF, \"label\", \"test\", False)\n",
    "\n",
    "# 14d.2. Persist\n",
    "blobName = f\"{modelBaseNm}/{operation}/{modelVersion}/full/metrics.json\"\n",
    "print(blobName)\n",
    "common_utils.fnPersistMetrics(urlparse(metricsBucketUri).netloc, modelMetricsWithTSP, blobName)\n",
    "\n",
    "# 14d.3. Print\n",
    "for m, v in modelMetricsWithTSP.items():\n",
    "    print(f'{m}: {v}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+-----------+-------------+---------------------+---------+\n",
      "|     metric_nm|metric_value|pipeline_id|model_version|pipeline_execution_dt|operation|\n",
      "+--------------+------------+-----------+-------------+---------------------+---------+\n",
      "| test_area_roc|     0.85105|       6451|         6451|       20240529131829| training|\n",
      "| test_area_prc|     0.66582|       6451|         6451|       20240529131829| training|\n",
      "| test_accuracy|     0.81228|       6451|         6451|       20240529131829| training|\n",
      "|       test_f1|     0.78894|       6451|         6451|       20240529131829| training|\n",
      "|test_precision|     0.80211|       6451|         6451|       20240529131829| training|\n",
      "|   test_recall|     0.81228|       6451|         6451|       20240529131829| training|\n",
      "+--------------+------------+-----------+-------------+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14e. Persist metrics subset to BigQuery\n",
    "metricsDF = spark.createDataFrame(modelMetrics.items(), [\"metric_nm\", \"metric_value\"]) \n",
    "metricsWithPipelineIdDF = metricsDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(operation)) \n",
    "\n",
    "metricsWithPipelineIdDF.show()\n",
    "\n",
    "metricsWithPipelineIdDF.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', bigQueryModelMetricsTableFQN) \\\n",
    ".save()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/agni-6\",\"uuid\":\"35fda7e3-be7b-4913-99c5-83e97b677386\",\"createTime\":\"2022-08-04T02:37:17.836903Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-04T02:38:37.084371Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-974925525028/35fda7e3-be7b-4913-99c5-83e97b677386/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-04T02:37:17.836903Z\"}]}",
  "serverless_spark_kernel_name": "remote-bc514a4a91cec988ad3c15a7-pyspark"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
