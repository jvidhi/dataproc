{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a212fa6e-2198-41c6-9e69-8d7abb407684",
   "metadata": {},
   "source": [
    "# MLOps with Spark MLLib & Vertex AI Pipelines\n",
    "In the prior lab modules - <br>(1) We authored Spark code in Serverless Spark interactive notebooks in Vertex AI workbench. <br>(2) We then created PySpark scripts off of them and tested them manually on cloud shell. <br> In this notebook -<br> (3) We will create a Vertex AI pipeline for MLOps - essentially chaining together the serverless Spark applications we developed in (2)<br> In a subsequent modules -<br>(4) We will create a Google Cloud Function to execute the pipeline and <br> (5) Finally, we will call that Google Cloud Function via Cloud Scheduler to complete the automation.\n",
    "We will compile the pipeline JSON and use the same to schedule with Cloud Scheduler separately.\n",
    "\n",
    "Dependency: Custom container image for Serverless Spark (already) created as part of (your) Terraform-based environment provisioning if you have created your environment using the Terraform provided in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5a414-f457-4e23-8c8e-62200dc6cfe7",
   "metadata": {},
   "source": [
    "### 1. One time setup of dependencies\n",
    "Uncomment the cell below, and run just the cell, one time ONLY to install necessary libraries - AND THEN comment it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c98b0-89c9-4eca-bd1d-a39ac780c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip3 install --user --upgrade google-cloud-aiplatform==1.11.0 kfp==1.8.11 google-cloud-pipeline-components==1.0.1 --quiet --no-warn-conflicts\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46e6a9-0c11-42fe-9595-3007cfcfb0ef",
   "metadata": {},
   "source": [
    "### 2. Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34ff675-62e7-410f-b619-f1f8d8ab0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path as path\n",
    "from typing import NamedTuple\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "# from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "# from kfp.v2 import compiler, dsl\n",
    "# from kfp.v2.dsl import (Artifact, ClassificationMetrics, Condition, Input,\n",
    "#                         Metrics, Output, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0d6d1a-74ae-46f1-b16d-2c4aacc3b065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import (Artifact, ClassificationMetrics, Condition, Input,Metrics, Output, component)\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,ModelDeployOp)\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a129398-d241-47b8-8063-73b605a936d8",
   "metadata": {},
   "source": [
    "#### a. Project specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed589bb-8ed3-4cb0-96c4-462a310d1741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vertex-ai-382806\n",
      "Project Number:  433578906282\n",
      "UMSA FQN:  433578906282-compute@developer.gserviceaccount.com\n",
      "UNIQUE ID:  3759\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"vertex-ai-382806\"\n",
    "PROJECT_NBR = \"433578906282\"\n",
    "UNIQUE_ID = random.randint(1, 10000)\n",
    "WITHOUT_TASK_CACHING = True\n",
    "BYO_NETWORK = True\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = project_id_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    \n",
    "    project_nbr_output = !gcloud projects describe $PROJECT_ID --format='value(projectNumber)'\n",
    "    PROJECT_NBR = project_nbr_output[0]\n",
    "    print(\"Project Number: \", PROJECT_NBR)\n",
    "    \n",
    "umsa_output = !gcloud config list account --format \"value(core.account)\"\n",
    "UMSA_FQN = umsa_output[0]\n",
    "print(\"UMSA FQN: \", UMSA_FQN)\n",
    "print(\"UNIQUE ID: \", UNIQUE_ID)\n",
    "\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f040cba-24ce-4550-93a7-b851d732f301",
   "metadata": {},
   "source": [
    "#### b. Local resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a231439c-6ae0-4847-8c3f-8ee23c4c5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_BASE_NM = \"customer-churn-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e1340b-3af1-4f95-a763-b8a1e7ca1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_SCRATCH_DIR = path(f\"/home/jupyter/scratch/{APP_BASE_NM}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760f8ece-197a-4c9e-be28-03619594bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca50a5dc-d66c-4ff7-a408-db038568ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 164\n",
      "drwxrwxrwx 2 jupyter jupyter  4096 Jun  3 06:26 .\n",
      "drwxr-xr-x 3 jupyter jupyter  4096 May 29 10:41 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 31298 May 29 12:47 pipeline_1257.json\n",
      "-rw-r--r-- 1 jupyter jupyter 25656 Jun  1 21:37 pipeline_1736.json\n",
      "-rw-r--r-- 1 jupyter jupyter 25811 Jun  2 09:04 pipeline_4032.json\n",
      "-rw-r--r-- 1 jupyter jupyter 33061 Jun  3 07:08 pipeline_8452.json\n",
      "-rw-r--r-- 1 jupyter jupyter 31298 Jun  1 14:48 pipeline_8899.json\n"
     ]
    }
   ],
   "source": [
    "!ls -al $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5a57b5-4cfc-48cb-9f88-cd437353b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/scratch/customer-churn-model\n"
     ]
    }
   ],
   "source": [
    "!cd $LOCAL_SCRATCH_DIR && pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593ff06b-8ac6-499d-a988-50b93d0e58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 104\n",
      "drwxr-xr-x 14 jupyter jupyter  4096 Jun  5 13:13 .\n",
      "drwxr-xr-x  3 root    root     4096 May 27 15:20 ..\n",
      "-rw-------  1 jupyter jupyter  2797 Jun  3 20:20 .bash_history\n",
      "-rw-r--r--  1 jupyter jupyter   431 May 27 15:21 .bashrc\n",
      "drwxr-xr-x  5 jupyter jupyter  4096 May 27 15:24 .cache\n",
      "drwxr-xr-x  4 jupyter jupyter  4096 May 27 15:21 .config\n",
      "drwxr-xr-x  2 jupyter jupyter  4096 May 29 11:01 .docker\n",
      "drwxr-xr-x  3 jupyter jupyter  4096 May 29 11:01 .gsutil\n",
      "drwxr-xr-x  2 jupyter jupyter  4096 Jun  4 14:24 .ipynb_checkpoints\n",
      "drwxr-xr-x  3 jupyter jupyter  4096 May 27 15:20 .ipython\n",
      "drwxr-xr-x  4 jupyter jupyter  4096 May 28 07:54 .jupyter\n",
      "drwxr-xr-x  2 jupyter jupyter  4096 May 28 08:52 .keras\n",
      "drwxr-xr-x  5 jupyter jupyter  4096 May 27 15:34 .local\n",
      "-rw-------  1 jupyter jupyter  1354 May 29 14:10 .viminfo\n",
      "-rw-r--r--  1 jupyter jupyter 24494 Jun  5 13:13 RunInference_test_beam.ipynb\n",
      "-rw-r--r--  1 jupyter jupyter  6264 Jun  3 12:28 beam_file.py\n",
      "drwxr-xr-x  2 jupyter jupyter  4096 Jun  1 19:52 build\n",
      "drwxr-xr-x  4 jupyter jupyter  4096 May 29 10:41 dataproc_serverless\n",
      "-rw-r--r--  1 jupyter jupyter    18 Jun  4 18:32 requirements.txt\n",
      "drwxr-xr-x  3 jupyter jupyter  4096 May 29 10:41 scratch\n"
     ]
    }
   ],
   "source": [
    "!ls -al /home/jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7b7a0-4281-4d6b-9cec-7fed2b300b44",
   "metadata": {},
   "source": [
    "#### d. The pre-created resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "347c9514-0d95-47f7-9671-1c2b5ff73255",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_BUCKET = f\"gs://s8s_code_bucket-{PROJECT_NBR}\"\n",
    "DATA_BUCKET = f\"gs://s8s_data_bucket-{PROJECT_NBR}\"\n",
    "MODEL_BUCKET = f\"gs://s8s_model_bucket-{PROJECT_NBR}\"\n",
    "SCRATCH_BUCKET = f\"s8s-spark-bucket-{PROJECT_NBR}\"\n",
    "BQ_DS_NM = f\"{PROJECT_ID}.customer_churn_ds\"\n",
    "LOCATION = \"us-central1\"\n",
    "VPC_NM = f\"s8s-vpc-{PROJECT_NBR}\"\n",
    "SUBNET_RESOURCE_URI = f\"projects/{PROJECT_ID}/regions/{LOCATION}/subnetworks/spark-snet\"\n",
    "PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI = f\"projects/{PROJECT_ID}/regions/{LOCATION}/clusters/s8s-sphs-{PROJECT_NBR}\"\n",
    "GCR_REPO_NM = f\"s8s-spark-{PROJECT_NBR}\"\n",
    "DOCKER_IMAGE_TAG = \"1.0.0\"\n",
    "DOCKER_IMAGE_NM = \"customer_churn_image\"\n",
    "DOCKER_IMAGE_FQN = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/s8s-spark/{DOCKER_IMAGE_NM}:{DOCKER_IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab3891-5f9d-40f3-b709-80754abe8d1d",
   "metadata": {},
   "source": [
    "#### e. Pipeline entity specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6858b5a-40bd-4705-9a77-2dc3fd97eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ID = 3759\n",
      "PIPELINE_NM = customer-churn-model-pipeline\n",
      "PIPELINE_PACKAGE_SRC_LOCAL_PATH = /home/jupyter/scratch/customer-churn-model/pipeline_3759.json\n",
      "PIPELINE_ROOT_GCS_URI = gs://s8s_model_bucket-433578906282/customer-churn-model/pipelines\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ID = UNIQUE_ID\n",
    "PIPELINE_NM = f\"{APP_BASE_NM}-pipeline\"\n",
    "PIPELINE_PACKAGE_SRC_LOCAL_PATH = f\"{LOCAL_SCRATCH_DIR}/pipeline_{PIPELINE_ID}.json\"\n",
    "PIPELINE_ROOT_GCS_URI = f\"{MODEL_BUCKET}/{APP_BASE_NM}/pipelines\"\n",
    "\n",
    "print('PIPELINE_ID =',PIPELINE_ID)\n",
    "print('PIPELINE_NM =',PIPELINE_NM)\n",
    "print('PIPELINE_PACKAGE_SRC_LOCAL_PATH =',PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "print('PIPELINE_ROOT_GCS_URI =',PIPELINE_ROOT_GCS_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a93fba-9714-46dc-b8f7-929331c159dc",
   "metadata": {},
   "source": [
    "#### d. Pipeline stage agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c437c615-18eb-4aa1-92ef-65a6af02a39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY_SCRIPTS_FQP = gs://s8s_code_bucket-433578906282/pyspark\n",
      "PYSPARK_COMMON_UTILS_SCRIPT_FQP = ['gs://s8s_code_bucket-433578906282/pyspark/common_utils.py']\n"
     ]
    }
   ],
   "source": [
    "DATAPROC_S8S_RUNTIME=\"2.1\"\n",
    "PY_SCRIPTS_FQP = f\"{CODE_BUCKET}/pyspark\"\n",
    "PYSPARK_COMMON_UTILS_SCRIPT_FQP = [f\"{PY_SCRIPTS_FQP}/common_utils.py\"]\n",
    "\n",
    "print('PY_SCRIPTS_FQP =',PY_SCRIPTS_FQP)\n",
    "print('PYSPARK_COMMON_UTILS_SCRIPT_FQP =',PYSPARK_COMMON_UTILS_SCRIPT_FQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf673b2e-89c6-4eaf-bb8b-69b206f84814",
   "metadata": {},
   "source": [
    "#### e. Data preprocessing stage specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d881fbc3-ee62-4969-928d-55a44dd2219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PREPROCESSING_BATCH_INSTANCE_ID = customer-churn-model-preprocessing-3759\n",
      "DATA_PREPROCESSING_MAIN_PY_SCRIPT = gs://s8s_code_bucket-433578906282/pyspark/preprocessing.py\n",
      "DATA_PROCESSING_SINK = vertex-ai-382806.customer_churn_ds.training_data\n",
      "DATA_PROCESSING_BQ_SINK_URI = bq://vertex-ai-382806.customer_churn_ds.training_data\n",
      "DATA_PREPROCESSING_ARGS = ['--pipelineID=3759', '--projectID=vertex-ai-382806', '--projectNbr=433578906282', '--displayPrintStatements=True']\n"
     ]
    }
   ],
   "source": [
    "DATA_PREPROCESSING_BATCH_PREFIX = \"preprocessing\"\n",
    "DATA_PREPROCESSING_BATCH_INSTANCE_ID = f\"{APP_BASE_NM}-{DATA_PREPROCESSING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "DATA_PREPROCESSING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/preprocessing.py\"\n",
    "\n",
    "DATA_PROCESSING_SINK = f\"{BQ_DS_NM}.training_data\"\n",
    "DATA_PROCESSING_BQ_SINK_URI = f\"bq://{DATA_PROCESSING_SINK}\"\n",
    "\n",
    "DATA_PREPROCESSING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "print('DATA_PREPROCESSING_BATCH_INSTANCE_ID =',DATA_PREPROCESSING_BATCH_INSTANCE_ID)\n",
    "print('DATA_PREPROCESSING_MAIN_PY_SCRIPT =',DATA_PREPROCESSING_MAIN_PY_SCRIPT)\n",
    "print('DATA_PROCESSING_SINK =',DATA_PROCESSING_SINK)\n",
    "print('DATA_PROCESSING_BQ_SINK_URI =',DATA_PROCESSING_BQ_SINK_URI)\n",
    "print('DATA_PREPROCESSING_ARGS =',DATA_PREPROCESSING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca656c-5ae8-45ab-95a3-873fede76c01",
   "metadata": {},
   "source": [
    "#### f. Dataset registration specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50eae873-c8e8-4f0b-9bac-e83c58f64ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANAGED_DATASET_NM = f\"{APP_BASE_NM}-{UNIQUE_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d69e4b-d088-4293-a88e-5b4d943615c8",
   "metadata": {},
   "source": [
    "#### g. Model specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecfd7e0e-3b57-4c4d-a148-4357bf816b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_TRAINING_BATCH_INSTANCE_ID = customer-churn-model-training-3759\n",
      "MODEL_TRAINING_MAIN_PY_SCRIPT = gs://s8s_code_bucket-433578906282/pyspark/model_training.py\n",
      "MODEL_TRAINING_ARGS = ['--pipelineID=3759', '--projectID=vertex-ai-382806', '--projectNbr=433578906282', '--displayPrintStatements=True']\n",
      "MODEL_METRICS_BUCKET_FQP = gs://s8s_metrics_bucket-433578906282/customer-churn-model/training/3759/full/metrics.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_TRAINING_BATCH_PREFIX = \"training\"\n",
    "MODEL_TRAINING_BATCH_INSTANCE_ID = f\"{APP_BASE_NM}-{MODEL_TRAINING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "MODEL_TRAINING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/model_training.py\"\n",
    "MODEL_TRAINING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "MODEL_METRICS_BUCKET_FQP = f\"gs://s8s_metrics_bucket-{PROJECT_NBR}/{APP_BASE_NM}/{MODEL_TRAINING_BATCH_PREFIX}/{UNIQUE_ID}/full/metrics.json\"\n",
    "\n",
    "print('MODEL_TRAINING_BATCH_INSTANCE_ID =',MODEL_TRAINING_BATCH_INSTANCE_ID)\n",
    "print('MODEL_TRAINING_MAIN_PY_SCRIPT =',MODEL_TRAINING_MAIN_PY_SCRIPT)\n",
    "print('MODEL_TRAINING_ARGS =',MODEL_TRAINING_ARGS)\n",
    "print('MODEL_METRICS_BUCKET_FQP =',MODEL_METRICS_BUCKET_FQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8f31c-44e3-44cc-8eb1-9eadf50c0137",
   "metadata": {},
   "source": [
    "#### h. Hyperparameter tuning specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bfbc78c-8d1d-4590-a86d-925b35c3fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID = customer-churn-model-hyperparameter-tuning-3759\n",
      "HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT = gs://s8s_code_bucket-433578906282/pyspark/hyperparameter_tuning.py\n",
      "HYPERPARAMETER_TUNING_ARGS = ['--pipelineID=3759', '--projectID=vertex-ai-382806', '--projectNbr=433578906282', '--displayPrintStatements=True']\n",
      "HYPERPARAMETER_TUNING_RUNTIME_CONFIGS = {'spark.jars.packages': 'ml.combust.mleap:mleap-spark-base_2.12:0.20.0,ml.combust.mleap:mleap-spark_2.12:0.20.0'}\n"
     ]
    }
   ],
   "source": [
    "# Condition\n",
    "AUPR_THRESHOLD = 0.5\n",
    "AUPR_HYPERTUNE_CONDITION = \"[AUPR_HYPERTUNE]\"\n",
    "\n",
    "HYPERPARAMETER_TUNING_BATCH_PREFIX = \"hyperparameter-tuning\"\n",
    "HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID = f\"{APP_BASE_NM}-{HYPERPARAMETER_TUNING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "HYPERPARAMETER_TUNING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/hyperparameter_tuning.py\"\n",
    "HYPERPARAMETER_TUNING_BUCKET_FQP = f\"gs://s8s_metrics_bucket-{PROJECT_NBR}/{APP_BASE_NM}/{HYPERPARAMETER_TUNING_BATCH_PREFIX}/{UNIQUE_ID}/full/metrics.json\"\n",
    "HYPERPARAMETER_TUNING_RUNTIME_CONFIGS = {\n",
    "    \"spark.jars.packages\": \"ml.combust.mleap:mleap-spark-base_2.12:0.20.0,ml.combust.mleap:mleap-spark_2.12:0.20.0\"\n",
    "}\n",
    "\n",
    "\n",
    "print('HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID =',HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID)\n",
    "print('HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT =',HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT)\n",
    "print('HYPERPARAMETER_TUNING_ARGS =',HYPERPARAMETER_TUNING_ARGS)\n",
    "print('HYPERPARAMETER_TUNING_RUNTIME_CONFIGS =',HYPERPARAMETER_TUNING_RUNTIME_CONFIGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e5090-1f28-4022-b3b1-82099710238b",
   "metadata": {},
   "source": [
    "### 3. Initialize Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9c54e20-c08a-40cf-8716-09de594df623",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=SCRATCH_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa4fd3-4b64-4946-a4ac-5937ad96a671",
   "metadata": {},
   "source": [
    "### 4. Define custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af80172f-b441-4104-98a0-3bf42dff4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"numpy==1.24.4\", \"pandas==2.2.2\", \"scikit-learn==0.24.2\"],\n",
    ")\n",
    "def fnEvaluateModel(\n",
    "    metricsUri: str,\n",
    "    metrics: Output[Metrics],\n",
    "    plots: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"threshold_metric\", float)]):\n",
    "\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "    # Variables\n",
    "    metricsGCSMountPath = metricsUri.replace(\"gs://\", \"/gcs/\")\n",
    "    labels = [\"yes\", \"no\"]\n",
    "\n",
    "    # Helpers\n",
    "    def fnCalculateROC(metrics, true, score):\n",
    "        y_true_np = np.array(metrics[true])\n",
    "        y_score_np = np.array(metrics[score])\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "            y_true=y_true_np, y_score=y_score_np, pos_label=True\n",
    "        )\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def fnCalculateConfusionMatrix(metrics, true, prediction):\n",
    "        y_true_np = np.array(metrics[true])\n",
    "        y_pred_np = np.array(metrics[prediction])\n",
    "        c_matrix = confusion_matrix(y_true_np, y_pred_np)\n",
    "        return c_matrix\n",
    "\n",
    "    # Main\n",
    "    with open(metricsGCSMountPath, mode=\"r\") as json_file:\n",
    "        metricsDictionary = json.load(json_file)\n",
    "\n",
    "    area_roc = metricsDictionary[\"test_area_roc\"]\n",
    "    area_prc = metricsDictionary[\"test_area_prc\"]\n",
    "    acc = metricsDictionary[\"test_accuracy\"]\n",
    "    f1 = metricsDictionary[\"test_f1\"]\n",
    "    prec = metricsDictionary[\"test_precision\"]\n",
    "    rec = metricsDictionary[\"test_recall\"]\n",
    "\n",
    "    metrics.log_metric(\"Test_areaUnderROC\", area_roc)\n",
    "    metrics.log_metric(\"Test_areaUnderPRC\", area_prc)\n",
    "    metrics.log_metric(\"Test_Accuracy\", acc)\n",
    "    metrics.log_metric(\"Test_f1-score\", f1)\n",
    "    metrics.log_metric(\"Test_Precision\", prec)\n",
    "    metrics.log_metric(\"Test_Recall\", rec)\n",
    "\n",
    "    fpr, tpr, thresholds = fnCalculateROC(metricsDictionary, \"true\", \"score\")\n",
    "    c_matrix = fnCalculateConfusionMatrix(metricsDictionary, \"true\", \"prediction\")\n",
    "    plots.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "    plots.log_confusion_matrix(labels, c_matrix.tolist())\n",
    "\n",
    "    componentOutputsTuple = NamedTuple(\n",
    "        \"Outputs\",\n",
    "        [\n",
    "            (\"threshold_metric\", float),\n",
    "        ],\n",
    "    )\n",
    "    return componentOutputsTuple(area_prc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3c111-17dc-4248-9df8-7c65f73c696c",
   "metadata": {},
   "source": [
    "### 5. Define Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902ec5e-54ec-4010-8f20-52e8c2ab6a6e",
   "metadata": {},
   "source": [
    "##### Option 1: In this version we dont disable task level caching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "346cdd5b-53a4-4760-b4f7-e595f7c20d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gcp_resources': {{channel:task=dataproc-create-pyspark-batch;name=gcp_resources;type=String;}}}\n",
      "{'dataset': {{channel:task=tabular-dataset-create;name=dataset;type=google.VertexDataset@0.0.1;}}}\n",
      "{'gcp_resources': {{channel:task=dataproc-create-pyspark-batch-2;name=gcp_resources;type=String;}}}\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NM, \n",
    "    description=\"A MLOps Vertex pipeline\")\n",
    "def fnSparkMlopsPipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    service_account: str = UMSA_FQN,\n",
    "    subnetwork_uri: str = SUBNET_RESOURCE_URI,\n",
    "    spark_phs_nm: str = PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI,\n",
    "    container_image: str = DOCKER_IMAGE_FQN,\n",
    "    common_utils_py_fqn: list = PYSPARK_COMMON_UTILS_SCRIPT_FQP,\n",
    "    data_preprocessing_pyspark_batch_id: str = DATA_PREPROCESSING_BATCH_INSTANCE_ID,\n",
    "    data_preprocessing_main_py_fqn: str = DATA_PREPROCESSING_MAIN_PY_SCRIPT,\n",
    "    data_preprocessing_args: list = DATA_PREPROCESSING_ARGS,\n",
    "    managed_dataset_display_nm: str = MANAGED_DATASET_NM,\n",
    "    managed_dataset_src_uri: str = DATA_PROCESSING_BQ_SINK_URI,\n",
    "    model_training_pyspark_batch_id: str = MODEL_TRAINING_BATCH_INSTANCE_ID,\n",
    "    model_training_main_py_fqn: str = MODEL_TRAINING_MAIN_PY_SCRIPT,\n",
    "    model_training_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    model_training_args: list = MODEL_TRAINING_ARGS,\n",
    "    threshold: float = AUPR_THRESHOLD,\n",
    "    hyperparameter_tuning_pyspark_batch_id: str = HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID,\n",
    "    hyperparameter_tuning_main_py_fqn: str = HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT,\n",
    "    hyperparameter_tuning_args: list = HYPERPARAMETER_TUNING_ARGS,\n",
    "    hyperparameter_tuning_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    hyperparameter_tuning_runtime_config_properties: dict = HYPERPARAMETER_TUNING_RUNTIME_CONFIGS,\n",
    "    dataproc_runtime_version: str = DATAPROC_S8S_RUNTIME\n",
    "):\n",
    "    from google_cloud_pipeline_components.v1.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    # Step 1. PRE-PROCESS DATA in PREP FOR MODEL TRAINING\n",
    "    # ....................................................................\n",
    "    preprocessingStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = data_preprocessing_pyspark_batch_id,\n",
    "        main_python_file_uri = data_preprocessing_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = data_preprocessing_args,\n",
    "        runtime_config_version = dataproc_runtime_version\n",
    "    ).set_display_name(\"Preprocessing\")\n",
    "    \n",
    "    print(preprocessingStep.outputs)\n",
    "    \n",
    "    from google_cloud_pipeline_components.v1.dataset import \\\n",
    "        TabularDatasetCreateOp\n",
    "    # Step 2. REGISTER PRE-PROCESSED DATA AS MANAGED DATASET\n",
    "    # ....................................................................\n",
    "    \n",
    "    createManagedDatasetStep = TabularDatasetCreateOp(\n",
    "        display_name= managed_dataset_display_nm,\n",
    "        bq_source=managed_dataset_src_uri,\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    ).after(preprocessingStep).set_display_name(\"Dataset registration\")\n",
    "    \n",
    "    print(createManagedDatasetStep.outputs)\n",
    "    \n",
    "    # Step 3. TRAIN MODEL\n",
    "    # .................................................................... \n",
    "    trainSparkMLModelStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = model_training_pyspark_batch_id,\n",
    "        main_python_file_uri = model_training_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = model_training_args,\n",
    "        runtime_config_properties = hyperparameter_tuning_runtime_config_properties,\n",
    "        runtime_config_version = dataproc_runtime_version\n",
    "    ).after(preprocessingStep).set_display_name(\"Model training\")\n",
    "    \n",
    "    print(trainSparkMLModelStep.outputs)\n",
    "    \n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from kfp.dsl import importer_node\n",
    "    \n",
    "    importer_spec = importer_node.importer(\n",
    "      artifact_uri='s8s_model_bucket-433578906282/customer-churn-model',\n",
    "      artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "      metadata={\n",
    "          'containerSpec': {\n",
    "              'imageUri':\n",
    "                  'us-central1-docker.pkg.dev/vertex-ai-382806/churn-tabular/prediction-server:prod'\n",
    "          }\n",
    "      })\n",
    "    \n",
    "    model_task = ModelUploadOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        display_name='spark-churn-model',\n",
    "        unmanaged_container_model=importer_spec.outputs[\"artifact\"],\n",
    "    ).after(trainSparkMLModelStep).set_display_name(\"Model Upload\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e6167-2446-4595-88ff-8d01880972ad",
   "metadata": {},
   "source": [
    "##### Option 2: In this version we disable task level caching\n",
    "Prefer this for scheduled execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb932bdd-8e1d-4a04-84bd-10bb6d7bd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NM, \n",
    "    description=\"A SparkMLlib MLOps Vertex pipeline\")\n",
    "def fnSparkMlopsPipelineWithoutCaching(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    service_account: str = UMSA_FQN,\n",
    "    subnetwork_uri: str = SUBNET_RESOURCE_URI,\n",
    "    spark_phs_nm: str = PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI,\n",
    "    container_image: str = DOCKER_IMAGE_FQN,\n",
    "    common_utils_py_fqn: list = PYSPARK_COMMON_UTILS_SCRIPT_FQP,\n",
    "    data_preprocessing_pyspark_batch_id: str = DATA_PREPROCESSING_BATCH_INSTANCE_ID,\n",
    "    data_preprocessing_main_py_fqn: str = DATA_PREPROCESSING_MAIN_PY_SCRIPT,\n",
    "    data_preprocessing_args: list = DATA_PREPROCESSING_ARGS,\n",
    "    managed_dataset_display_nm: str = MANAGED_DATASET_NM,\n",
    "    managed_dataset_src_uri: str = DATA_PROCESSING_BQ_SINK_URI,\n",
    "    model_training_pyspark_batch_id: str = MODEL_TRAINING_BATCH_INSTANCE_ID,\n",
    "    model_training_main_py_fqn: str = MODEL_TRAINING_MAIN_PY_SCRIPT,\n",
    "    model_training_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    model_training_args: list = MODEL_TRAINING_ARGS,\n",
    "    threshold: float = AUPR_THRESHOLD,\n",
    "    hyperparameter_tuning_pyspark_batch_id: str = HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID,\n",
    "    hyperparameter_tuning_main_py_fqn: str = HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT,\n",
    "    hyperparameter_tuning_args: list = HYPERPARAMETER_TUNING_ARGS,\n",
    "    hyperparameter_tuning_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    hyperparameter_tuning_runtime_config_properties: dict = HYPERPARAMETER_TUNING_RUNTIME_CONFIGS,\n",
    "    dataproc_runtime_version: str = DATAPROC_S8S_RUNTIME\n",
    "):\n",
    "    from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp\n",
    "\n",
    "    # Step 1. PRE-PROCESS DATA in PREP FOR MODEL TRAINING\n",
    "    # ....................................................................\n",
    "    preprocessingStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = data_preprocessing_pyspark_batch_id,\n",
    "        main_python_file_uri = data_preprocessing_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = data_preprocessing_args,\n",
    "        runtime_config_version = dataproc_runtime_version\n",
    "    ).set_caching_options(False).set_display_name(\"Preprocessing\")\n",
    "    \n",
    "    from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "    # Step 2. REGISTER PRE-PROCESSED DATA AS MANAGED DATASET\n",
    "    # ....................................................................\n",
    "    createManagedDatasetStep = TabularDatasetCreateOp(\n",
    "        display_name= managed_dataset_display_nm,\n",
    "        bq_source=managed_dataset_src_uri,\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    ).after(preprocessingStep).set_caching_options(False).set_display_name(\"Dataset registration\")\n",
    "    \n",
    "    # Step 3. TRAIN MODEL\n",
    "    # .................................................................... \n",
    "    trainSparkMLModelStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = model_training_pyspark_batch_id,\n",
    "        main_python_file_uri = model_training_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = model_training_args,\n",
    "        runtime_config_version = dataproc_runtime_version,\n",
    "        runtime_config_properties = hyperparameter_tuning_runtime_config_properties,\n",
    "    ).set_caching_options(False).after(preprocessingStep).set_display_name(\"Model training\")\n",
    "    \n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from kfp.dsl import importer_node\n",
    "\n",
    "    # Step 3. UPLOAD THE MODEL\n",
    "    importer_spec = importer_node.importer(\n",
    "      artifact_uri='gs://s8s_model_bucket-433578906282/customer-churn-model',\n",
    "      artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "      metadata={\n",
    "          'containerSpec': {\n",
    "              'imageUri':\n",
    "                  'us-central1-docker.pkg.dev/vertex-ai-382806/churn-tabular/prediction-server:prod'\n",
    "          }\n",
    "      }).after(trainSparkMLModelStep).set_display_name(\"Model Upload\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126d252-8cad-4615-bb65-1126792ef9e7",
   "metadata": {},
   "source": [
    "### 5. Compile the Vertex AI Pipeline into a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb6ecfbc-dcad-4848-9acf-de6687c91701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing fnSparkMlopsPipelineWithoutCaching\n"
     ]
    }
   ],
   "source": [
    "if WITHOUT_TASK_CACHING:\n",
    "    compiler.Compiler().compile(pipeline_func=fnSparkMlopsPipelineWithoutCaching, package_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "    print(\"Executing fnSparkMlopsPipelineWithoutCaching\")\n",
    "else:\n",
    "    compiler.Compiler().compile(pipeline_func=fnSparkMlopsPipeline, package_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "    print(\"Executing fnSparkMlopsPipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f665f0d-7a95-4dfb-b210-a959a3a3f914",
   "metadata": {},
   "source": [
    "### 6. Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156f8be4-5369-4078-8191-96ca42eeb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NM,\n",
    "    template_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT_GCS_URI,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd619ec-ecf7-4318-991a-b9330c1f51e3",
   "metadata": {},
   "source": [
    "### 7. Submit the Pipeline for execution\n",
    "There are two options below, one that uses the customer specified network that is peered with Vertex AI tenant network, and one that uses Vertex AI tenant network altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93c96b7f-ab01-4546-9eec-d53aa1800c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/433578906282/locations/us-central1/pipelineJobs/customer-churn-model-pipeline-20240605131604\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/433578906282/locations/us-central1/pipelineJobs/customer-churn-model-pipeline-20240605131604')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/customer-churn-model-pipeline-20240605131604?project=433578906282\n"
     ]
    }
   ],
   "source": [
    "if BYO_NETWORK:\n",
    "    pipeline.submit(service_account=UMSA_FQN, network=f\"projects/{PROJECT_NBR}/global/networks/{VPC_NM}\")\n",
    "else:\n",
    "    pipeline.submit(service_account=UMSA_FQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c390bda9-789f-4e4a-be1e-140ec3ee3c65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.24.4\n",
      "Summary: Fundamental package for array computing in Python\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: BSD-3-Clause\n",
      "Location: /opt/conda/envs/tensorflow/lib/python3.10/site-packages\n",
      "Requires: \n",
      "Required-by: apache-beam, contourpy, db-dtypes, explainable-ai-sdk, gymnasium, h5py, ImageHash, imageio, jax-jumpy, matplotlib, numba, opt-einsum, pandas, patsy, phik, pyarrow, PyWavelets, scikit-image, scikit-learn, scipy, seaborn, shapely, statsmodels, tensorboard, tensorboardX, tensorflow, tensorflow-datasets, tensorflow-hub, tensorflow-probability, tensorflow-transform, tifffile, visions, wordcloud, ydata-profiling\n"
     ]
    }
   ],
   "source": [
    "!pip show --version numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b5e66-c87a-40d1-b607-80a40260c07c",
   "metadata": {},
   "source": [
    "### 8. How do we automate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5de77-129a-411e-89bf-5d34cba0895b",
   "metadata": {},
   "source": [
    "To automate this, we will take the compiled JSON, (1) test run it via Vertex AI Cloud Console UI, and then (2) schedule with Cloud Scheduler (calls Cloud Function that calls the Vertex AI REST endpoint for the Vertex AI pipeline).<br>This is a lab module that you can proceed to next.<br>\n",
    "Note: The json below has your project details. The lab author has a de-identified version with placeholders for your information in the git repo ../04-templates/\\*.json and as part of the Terraform automation, a customized version except custom pipeline ID is placed in 05-pipelines in your local directory in cloud shell. A copy of it is placed in GCS -> s8s-pipelines-bucket-YOUR_PROJECT_NUMBER/templates. At scheduled run time, a new custom pipeline ID is generated in the cloud function, and substituted in the json and placed in the bucket - s8s-pipelines-bucket-YOUR_PROJECT_NUMBER/execution and this is used to launch the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eadd022-cd68-4ab4-9be7-bcf7a2e4f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat $LOCAL_SCRATCH_DIR/pipeline_${PIPELINE_ID}.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
